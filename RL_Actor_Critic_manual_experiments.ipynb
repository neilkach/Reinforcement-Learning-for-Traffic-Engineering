{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "H = 10 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "render = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor model initialization\n",
    "R = 3\n",
    "C = 4\n",
    "D = R * C # input dimensionality: delays, costs and user preferences for 4 tunnels\n",
    "if resume:\n",
    "  actor = pickle.load(open('actor_saved.p', 'rb'))\n",
    "else:\n",
    "  actor = {}\n",
    "  actor['W1'] = np.random.randn(H,R) / np.sqrt(R) # \"Xavier\" initialization\n",
    "  actor['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "\n",
    "actor_grad_buffer = { k : np.zeros_like(v) for k,v in actor.items() } # update buffers that add up gradients over a batch\n",
    "actor_rmsprop_cache = { k : np.zeros_like(v) for k,v in actor.items() } # rmsprop memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic model initialization\n",
    "R = 4\n",
    "C = 4\n",
    "D = R * C # input dimensionality: delays, costs and user preferences for 4 tunnels\n",
    "if resume:\n",
    "  critic = pickle.load(open('critic_saved.p', 'rb'))\n",
    "else:\n",
    "  critic = {}\n",
    "  critic['W1'] = np.random.randn(H,R) / np.sqrt(R) # \"Xavier\" initialization\n",
    "  critic['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "  critic['W3'] = np.random.randn(C) / np.sqrt(C)\n",
    "\n",
    "critic_grad_buffer = { k : np.zeros_like(v) for k,v in critic.items() } # update buffers that add up gradients over a batch\n",
    "critic_rmsprop_cache = { k : np.zeros_like(v) for k,v in critic.items() } # rmsprop memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(layer):\n",
    "  for i in range(len(layer)):\n",
    "    layer[i] = 1.0 / (1.0 + np.exp(-layer[i]))\n",
    "  return layer # sigmoid \"squashing\" function for each to input to interval [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(x):\n",
    "  h = actor['W1'] @ x\n",
    "  h[h<0] = 0 # ReLU nonlinearity, h is 10x4\n",
    "  weights = actor['W2'] @ h # weights is 1x4\n",
    "  w = sigmoid(weights)\n",
    "  return w, h # return new link weights and hidden state\n",
    "\n",
    "def actor_update(epx, eph, epdlogp):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  dW2 = np.dot(eph.T, epdlogp).ravel() # ep x ep --> 1 x H ??\n",
    "  dh = np.outer(epdlogp, actor['W2']) # ep x H\n",
    "  dh[eph <= 0] = 0 # backprop relu\n",
    "  dW1 = np.dot(dh.T, epx) # Hx1\n",
    "  return {'W1':dW1, 'W2':dW2}\n",
    "\n",
    "def critique(x):\n",
    "  h = critic['W1'] @ x\n",
    "  h[h<0] = 0 # ReLU nonlinearity, h is 10x4\n",
    "  h2 = critic['W2'] @ h # h2 is 1x4\n",
    "  h2[h2<0] = 0 # ReLU nonlinearity\n",
    "  V = critic['W3'] @ h2 # V is 1x1\n",
    "  return V, h, h2 # return value and hidden state\n",
    "\n",
    "def critic_update(epx, eph, eph2, epdV):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  #output layer gradients\n",
    "  dW3 = np.dot(eph2.T, epdV).ravel()\n",
    "  #second hidden layer\n",
    "  dh2 = np.dot(epdV, critic['W3']) #10, 4\n",
    "  dh2[eph2.reshape(-1) <= 0] = 0 # backprop relu\n",
    "  dW2 = np.dot(eph, dh2.T).ravel() # (100,4) x (4,10) = (100,10) --> (1,1000)\n",
    "  #first hidden layer\n",
    "  dh = np.outer(critic['W2'], dh2.reshape(-1)).reshape(-1, 4)\n",
    "  dh[eph <= 0] = 0 # backprop relu\n",
    "  dW1 = np.dot(epx, dh.T)\n",
    "  return {'W1':dW1.T, 'W2':dW2, 'W3':dW3}\n",
    "\n",
    "def calc_reward(w, x):\n",
    "  user_pref = (w[0]>w[1] and w[0]>w[2] and w[0]>w[3]) + 0.75*(w[1]>w[2] and w[1]>w[3]) + 0.5*(w[2]>w[3]) \n",
    "  cost = - np.dot(w, x[1])\n",
    "  net_perf = np.dot(w, x[0]) \n",
    "  return user_pref + cost + net_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_x = None # used in computing the difference frame\n",
    "axs,ahs,das,rs = [],[],[],[]\n",
    "cxs,chs,ch2s,dqs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_x = np.array([[100,40,50,79],\n",
    "     [2,1,3,4]]) # first row for costs, second is preference\n",
    "iters = 0\n",
    "while True:\n",
    "     raw_weights = np.random.rand(1,4)\n",
    "     x = np.vstack((raw_weights, fixed_x))\n",
    "\n",
    "     # forward actor network\n",
    "     a, h = act(x) #action, and hidden state\n",
    "\n",
    "     # record intermediates\n",
    "     axs.append(x) # observation\n",
    "     ahs.append(h) # hidden state\n",
    "\n",
    "     # calculate reward from action and state\n",
    "     reward = calc_reward(a, x)\n",
    "\n",
    "     rs.append(reward) # needed?\n",
    "\n",
    "     # stack state and action for input into critic network\n",
    "     critic_input = np.vstack((x, a))\n",
    "     Q, ch, ch2 = critique(critic_input)\n",
    "\n",
    "     cxs.append(critic_input)\n",
    "     chs.append(ch)\n",
    "     ch2s.append(ch2)\n",
    "     dqs.append(reward - Q)\n",
    "\n",
    "\n",
    "     iters += 1\n",
    "     # if iters == 10: # episode ends\n",
    "     #      iters = 0\n",
    "     #      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "     #      epax = np.vstack(axs)\n",
    "     #      epah = np.vstack(ahs)\n",
    "     #      #epda = np.vstack(das)\n",
    "     #      epr = np.vstack(rs)\n",
    "     #      epcx = np.vstack(cxs)\n",
    "     #      epch = np.vstack(chs)\n",
    "     #      epch2 = np.vstack(ch2s)\n",
    "     #      epdq = np.vstack(dqs)\n",
    "\n",
    "     #      #reset memory\n",
    "     #      axs,ahs,das,rs = [],[],[],[]\n",
    "     #      cxs,chs,ch2s,dqs = [],[],[],[]\n",
    "\n",
    "          # get the gradient from the critic\n",
    "          #critic_grad = critic_update(epcx, epch, epch2, epdq)\n",
    "     actor_grad = actor_update(x, h, -Q)\n",
    "     for k in actor: actor_grad_buffer[k] += actor_grad[k]\n",
    "     for k,v in actor.items():\n",
    "          g = actor_grad_buffer[k]\n",
    "          actor_rmsprop_cache[k] = decay_rate * actor_rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "          actor[k] += learning_rate * g / (np.sqrt(actor_rmsprop_cache[k]) + 1e-5)\n",
    "          actor_grad_buffer[k] = np.zeros_like(v)\n",
    "\n",
    "     critic_grad = critic_update(critic_input, ch, ch2, reward - Q) # i think i need to adjust this error to the TD error\n",
    "     for k in critic: critic_grad_buffer[k] += critic_grad[k]\n",
    "     for k,v in critic.items():\n",
    "          g = critic_grad_buffer[k]\n",
    "          critic_rmsprop_cache[k] = decay_rate * critic_rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "          critic[k] += learning_rate * g / (np.sqrt(critic_rmsprop_cache[k]) + 1e-5)\n",
    "          critic_grad_buffer[k] = np.zeros_like(v)\n",
    "     #how to define error for backprop: reward - expected reward (critic output, i.e. Q value)\n",
    "     #forward state + action through critic network. more efficient way than manual backprop?\n",
    "     if iters%100==0: print(critic['W1'], critic['W2'], critic['W3'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
